# -*- coding: utf-8 -*-
"""Predictor.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Wy6sj_pwFDr3ET8Yw5hY0sq7exGgWT66
"""

import numpy as np
from imblearn.under_sampling import NeighbourhoodCleaningRule
import pandas

import keras
from keras.models import Sequential
from keras.layers import Dense

from keras.models import Sequential
from keras.layers import Dense
import tensorflow as tf

from keras.layers import Dense
# from imblearn.under_sampling import NeighbourhoodCleaningRule
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectFromModel
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

import matplotlib.pyplot as plt
import lime.lime_tabular

# Load dataset
dataset = pandas.read_csv("./Data/Supplementary_Table_S1.csv", ";")
print("Dataset imported")

x = dataset.iloc[:, 1:640]
print(x)
# print(x)
y = dataset['Class']
print(y)

clf = RandomForestClassifier(n_estimators=300, max_depth=9,
                             random_state=0)
clf.fit(x, y)
clf.feature_importances_
model = SelectFromModel(clf, prefit=True)
x = model.transform(x)
x.shape

ncr = NeighbourhoodCleaningRule()
x_resampled, y_resampled = ncr.fit_resample(x, y)

X_train, X_test, y_train, y_test = train_test_split(x_resampled, y_resampled, test_size=0.2, random_state=1)

print(x_resampled)
print("Hello")
print(y_resampled)

# Optimization Algorithm
opt = tf.keras.optimizers.RMSprop(learning_rate=0.00014, rho=0.9, epsilon=None, decay=0.0)

# Multi Layer Perceptron Model
model = Sequential()
model.add(Dense(128, input_dim=193, activation='relu'))
keras.layers.AlphaDropout(0.3, noise_shape=None, seed=None)

model.add(Dense(128, activation='relu'))
keras.layers.AlphaDropout(0.3, noise_shape=None, seed=None)

model.add(Dense(128, activation='relu'))
keras.layers.AlphaDropout(0.3, noise_shape=None, seed=None)

model.add(Dense(128, activation='relu'))
keras.layers.AlphaDropout(0.3, noise_shape=None, seed=None)

model.add(Dense(128, activation='relu'))
keras.layers.AlphaDropout(0.3, noise_shape=None, seed=None)

model.add(Dense(128, activation='relu'))
keras.layers.AlphaDropout(0.3, noise_shape=None, seed=None)

model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy',
              optimizer=opt,
              metrics=['accuracy'])

# Estimator

history = model.fit(X_train, y_train, epochs=200, validation_data=(X_test, y_test))
#model.load_weights('./checkpoints/a')
#score = model.evaluate(x_resampled, y_resampled)
#print(score)

print(history.history.keys())

yhat = model.predict(X_test)
print("predictions")
print(yhat)
classes = np.argmax(yhat, axis = 1)

acc = accuracy_score(y_test, classes)
print('Accuracy: %.3f' % acc)

loss_train = history.history['loss']
loss_val = history.history['val_loss']
epochs = range(0,200)
plt.plot(epochs, loss_train, 'g', label='Training loss')
plt.plot(epochs, loss_val, 'b', label='validation loss')
plt.title('Training and Validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

loss_train = history.history['accuracy']
loss_val = history.history['val_accuracy']
epochs = range(0,200)
plt.plot(epochs, loss_train, 'g', label='Training accuracy')
plt.plot(epochs, loss_val, 'b', label='validation accuracy')
plt.title('Training and Validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

explainer = lime.lime_tabular.LimeTabularExplainer(X_train, feature_names=list(dataset), class_names=[0, 1], mode='classification')
# added top_labels=1
exp = explainer.explain_instance(X_test[2], model.predict, labels=(0,))
# myMap = exp.as_map()
# print(myMap)

exp.as_pyplot_figure(label=0)
plt.show()
#print(y_test)
#print(yhat)

# save model for future use
#model.save_weights('./checkpoints/a')

# Predictor
# f = np.expand_dims(f, axis=0)
#f = ["aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa", "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA"]
#preds = model.predict(f)
#model.predict_classes(f, batch_size=1, verbose=1)

# Print the Results
#print(preds)
# print(pp)
